{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging using modified Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objectives and Plan for Assignment\n",
    "\n",
    "###### Step 1 : Write Vanilla Viterbi Algorithm \n",
    "- Write Vanilla Viterbi Algorithm for POS tagging using Treebank data with Universal tagset\n",
    "- Train - Validation set should be kept as 95:5 \n",
    "- Calculate tagging accuracy using Test Sentence file\n",
    "\n",
    "\n",
    "###### Step 2: Rule Based\n",
    "- Write a function for improving Vanilla viterbi by either modifying the entire function or calling another function for rule based modification for unkown words\n",
    "- compare the tagging accuracy on test file\n",
    "\n",
    "###### Step 3: Probabilistic \n",
    "- wtite a method to improve tagging by making state prob = emission prob for unknown words\n",
    "- compare the tagging accuracy\n",
    "\n",
    "###### Step 4: Sight 3 improvements in tagging test file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NOUN'),\n",
       "  ('Vinken', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('61', 'NUM'),\n",
       "  ('years', 'NOUN'),\n",
       "  ('old', 'ADJ'),\n",
       "  (',', '.'),\n",
       "  ('will', 'VERB'),\n",
       "  ('join', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('board', 'NOUN'),\n",
       "  ('as', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('nonexecutive', 'ADJ'),\n",
       "  ('director', 'NOUN'),\n",
       "  ('Nov.', 'NOUN'),\n",
       "  ('29', 'NUM'),\n",
       "  ('.', '.')],\n",
       " [('Mr.', 'NOUN'),\n",
       "  ('Vinken', 'NOUN'),\n",
       "  ('is', 'VERB'),\n",
       "  ('chairman', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('Elsevier', 'NOUN'),\n",
       "  ('N.V.', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('Dutch', 'NOUN'),\n",
       "  ('publishing', 'VERB'),\n",
       "  ('group', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('Rudolph', 'NOUN'),\n",
       "  ('Agnew', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('55', 'NUM'),\n",
       "  ('years', 'NOUN'),\n",
       "  ('old', 'ADJ'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('former', 'ADJ'),\n",
       "  ('chairman', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('Consolidated', 'NOUN'),\n",
       "  ('Gold', 'NOUN'),\n",
       "  ('Fields', 'NOUN'),\n",
       "  ('PLC', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('was', 'VERB'),\n",
       "  ('named', 'VERB'),\n",
       "  ('*-1', 'X'),\n",
       "  ('a', 'DET'),\n",
       "  ('nonexecutive', 'ADJ'),\n",
       "  ('director', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('this', 'DET'),\n",
       "  ('British', 'ADJ'),\n",
       "  ('industrial', 'ADJ'),\n",
       "  ('conglomerate', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('A', 'DET'),\n",
       "  ('form', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('asbestos', 'NOUN'),\n",
       "  ('once', 'ADV'),\n",
       "  ('used', 'VERB'),\n",
       "  ('*', 'X'),\n",
       "  ('*', 'X'),\n",
       "  ('to', 'PRT'),\n",
       "  ('make', 'VERB'),\n",
       "  ('Kent', 'NOUN'),\n",
       "  ('cigarette', 'NOUN'),\n",
       "  ('filters', 'NOUN'),\n",
       "  ('has', 'VERB'),\n",
       "  ('caused', 'VERB'),\n",
       "  ('a', 'DET'),\n",
       "  ('high', 'ADJ'),\n",
       "  ('percentage', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('cancer', 'NOUN'),\n",
       "  ('deaths', 'NOUN'),\n",
       "  ('among', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('group', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('workers', 'NOUN'),\n",
       "  ('exposed', 'VERB'),\n",
       "  ('*', 'X'),\n",
       "  ('to', 'PRT'),\n",
       "  ('it', 'PRON'),\n",
       "  ('more', 'ADV'),\n",
       "  ('than', 'ADP'),\n",
       "  ('30', 'NUM'),\n",
       "  ('years', 'NOUN'),\n",
       "  ('ago', 'ADP'),\n",
       "  (',', '.'),\n",
       "  ('researchers', 'NOUN'),\n",
       "  ('reported', 'VERB'),\n",
       "  ('0', 'X'),\n",
       "  ('*T*-1', 'X'),\n",
       "  ('.', '.')],\n",
       " [('The', 'DET'),\n",
       "  ('asbestos', 'NOUN'),\n",
       "  ('fiber', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('crocidolite', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('is', 'VERB'),\n",
       "  ('unusually', 'ADV'),\n",
       "  ('resilient', 'ADJ'),\n",
       "  ('once', 'ADP'),\n",
       "  ('it', 'PRON'),\n",
       "  ('enters', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('lungs', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('with', 'ADP'),\n",
       "  ('even', 'ADV'),\n",
       "  ('brief', 'ADJ'),\n",
       "  ('exposures', 'NOUN'),\n",
       "  ('to', 'PRT'),\n",
       "  ('it', 'PRON'),\n",
       "  ('causing', 'VERB'),\n",
       "  ('symptoms', 'NOUN'),\n",
       "  ('that', 'DET'),\n",
       "  ('*T*-1', 'X'),\n",
       "  ('show', 'VERB'),\n",
       "  ('up', 'PRT'),\n",
       "  ('decades', 'NOUN'),\n",
       "  ('later', 'ADJ'),\n",
       "  (',', '.'),\n",
       "  ('researchers', 'NOUN'),\n",
       "  ('said', 'VERB'),\n",
       "  ('0', 'X'),\n",
       "  ('*T*-2', 'X'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can see that nltk_data is a list of sentences tagged with POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3718\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "# Dividing nltk_data into train and validation set in ratio of 95:5\n",
    "random.seed(1234)\n",
    "train_set, valid_set = train_test_split(nltk_data,test_size=0.05)\n",
    "print(len(train_set))\n",
    "print(len(valid_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95782"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting list of tagged words\n",
    "train_tagged_words = [tup for sent in train_set for tup in sent]\n",
    "len(train_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('In', 'ADP'),\n",
       " ('August', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Temple', 'NOUN'),\n",
       " ('sweetened', 'VERB')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged_words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total training tokens \n",
    "train_tokens = [pair[0] for pair in train_tagged_words]\n",
    "# total training tags \n",
    "train_tags = [pair[1] for pair in train_tagged_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12067\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# unique words in train_set\n",
    "V = set(train_tokens)\n",
    "print(len(V))\n",
    "\n",
    "# unique tags in train set\n",
    "T = set(train_tags)\n",
    "print(len(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, there are 12112 unique words in training set\n",
    "- and there are 12 different tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PRT',\n",
       " 'VERB',\n",
       " 'X'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ADP': 9375,\n",
       "         'NOUN': 27402,\n",
       "         '.': 11143,\n",
       "         'VERB': 12908,\n",
       "         'DET': 8358,\n",
       "         'PRT': 3078,\n",
       "         'NUM': 3346,\n",
       "         'X': 6298,\n",
       "         'CONJ': 2161,\n",
       "         'ADJ': 6087,\n",
       "         'PRON': 2606,\n",
       "         'ADV': 3020})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding most common tags in training set\n",
    "tag_counts = Counter(train_tags)\n",
    "tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NOUN', 27402)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_counts.most_common()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tag\tMeaning\tEnglish Examples\n",
    "- ADJ\tadjective\tnew, good, high, special, big, local\n",
    "- ADP\tadposition\ton, of, at, with, by, into, under\n",
    "- ADV\tadverb\treally, already, still, early, now\n",
    "- CONJ\tconjunction\tand, or, but, if, while, although\n",
    "- DET\tdeterminer, article\tthe, a, some, most, every, no, which\n",
    "- NOUN\tnoun\tyear, home, costs, time, Africa\n",
    "- NUM\tnumeral\ttwenty-four, fourth, 1991, 14:24\n",
    "- PRT\tparticle\tat, on, out, over per, that, up, with\n",
    "- PRON\tpronoun\the, their, her, its, my, I, us\n",
    "- VERB\tverb\tis, say, told, given, playing, would\n",
    "- .\tpunctuation marks\t. , ; !\n",
    "- X\tother\tersatz, esprit, dunno, gr8, univeristy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vanilla Viterbi based POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emission Probability\n",
    "\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "    count_tag = len(tag_list)\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    "    \n",
    "    return (count_w_given_tag, count_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n",
    "\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==t1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating t x t transition matrix of tags\n",
    "# each column is t2, each row is t1\n",
    "# thus M(i, j) represents P(tj given ti)\n",
    "\n",
    "tags_matrix = np.zeros((len(T), len(T)), dtype='float32')\n",
    "for i, t1 in enumerate(list(T)):\n",
    "    for j, t2 in enumerate(list(T)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.70666669e-02, 3.25120002e-01, 3.54133323e-02, 6.98666647e-02,\n",
       "        8.42666626e-03, 3.20426673e-01, 1.05279997e-01, 3.94666679e-02,\n",
       "        6.30400032e-02, 1.49333337e-03, 1.36533333e-02, 7.46666687e-04],\n",
       "       [9.57166776e-03, 5.38406335e-03, 4.53457758e-02, 3.70902126e-03,\n",
       "        3.90045457e-02, 6.37592733e-01, 2.05671206e-01, 1.77075863e-02,\n",
       "        2.24934202e-02, 2.39291694e-04, 1.28021054e-02, 4.78583388e-04],\n",
       "       [1.42902508e-01, 5.50968572e-02, 7.52619877e-02, 5.52556366e-02,\n",
       "        2.03715459e-01, 6.16068579e-02, 1.76246427e-02, 1.64020330e-01,\n",
       "        1.90536678e-03, 1.86408386e-01, 2.61987932e-02, 1.00031756e-02],\n",
       "       [2.30237916e-02, 8.82578641e-03, 8.86415988e-02, 7.67459720e-03,\n",
       "        4.85418260e-01, 2.13353798e-01, 7.36761317e-02, 4.06753644e-02,\n",
       "        7.29086716e-03, 1.22793550e-02, 3.41519564e-02, 4.98848828e-03],\n",
       "       [9.06414613e-02, 1.36117131e-01, 2.17926860e-01, 3.60241719e-02,\n",
       "        1.68887511e-01, 1.09621942e-01, 6.46885633e-02, 3.47846299e-02,\n",
       "        2.27765720e-02, 3.16083059e-02, 8.16547871e-02, 5.26805082e-03],\n",
       "       [1.77578285e-01, 1.31377270e-02, 2.90489737e-02, 4.45222994e-03,\n",
       "        1.46595135e-01, 2.63374925e-01, 1.22253848e-02, 2.40529895e-01,\n",
       "        9.34238359e-03, 4.39383984e-02, 1.71885267e-02, 4.25881334e-02],\n",
       "       [7.80351534e-02, 5.09282062e-03, 2.10284218e-02, 4.92853636e-04,\n",
       "        1.23213409e-02, 6.98537886e-01, 6.65352419e-02, 6.45638257e-02,\n",
       "        2.08641365e-02, 1.11713484e-02, 4.76425188e-03, 1.65927391e-02],\n",
       "       [9.05501246e-02, 1.74728528e-01, 2.72817016e-02, 6.59606904e-02,\n",
       "        8.86655301e-02, 2.21215114e-01, 4.43327650e-02, 9.29731652e-02,\n",
       "        8.04092288e-02, 2.42304592e-03, 5.20506166e-02, 5.93197532e-02],\n",
       "       [3.49671245e-02, 3.28750745e-03, 2.12791398e-01, 1.49432162e-03,\n",
       "        1.85295884e-02, 3.52361023e-01, 3.46682593e-02, 1.17154814e-01,\n",
       "        1.81111768e-01, 2.63000596e-02, 2.98864325e-03, 1.43454876e-02],\n",
       "       [2.01429501e-02, 1.03963614e-01, 1.39701106e-02, 1.78687461e-02,\n",
       "        4.02209222e-01, 2.45938927e-01, 8.41455460e-02, 4.19103317e-02,\n",
       "        5.55555560e-02, 1.94931775e-03, 1.00714751e-02, 2.27420405e-03],\n",
       "       [1.19536422e-01, 6.75496683e-02, 2.25165561e-02, 1.49006620e-02,\n",
       "        3.48344386e-01, 3.01324502e-02, 1.27814576e-01, 1.38741717e-01,\n",
       "        3.07947025e-02, 1.45695368e-02, 7.81456977e-02, 6.95364224e-03],\n",
       "       [5.27533554e-02, 1.22628413e-01, 8.32947716e-03, 5.64553440e-02,\n",
       "        1.58722818e-01, 3.46598804e-01, 1.14761688e-01, 3.56316529e-02,\n",
       "        4.30356301e-02, 5.09023620e-03, 5.55298477e-02, 4.62748721e-04]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADP</th>\n",
       "      <th>DET</th>\n",
       "      <th>X</th>\n",
       "      <th>PRON</th>\n",
       "      <th>VERB</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>.</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PRT</th>\n",
       "      <th>ADV</th>\n",
       "      <th>CONJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.017067</td>\n",
       "      <td>0.325120</td>\n",
       "      <td>0.035413</td>\n",
       "      <td>0.069867</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.320427</td>\n",
       "      <td>0.105280</td>\n",
       "      <td>0.039467</td>\n",
       "      <td>0.063040</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.013653</td>\n",
       "      <td>0.000747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.009572</td>\n",
       "      <td>0.005384</td>\n",
       "      <td>0.045346</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.039005</td>\n",
       "      <td>0.637593</td>\n",
       "      <td>0.205671</td>\n",
       "      <td>0.017708</td>\n",
       "      <td>0.022493</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.012802</td>\n",
       "      <td>0.000479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.142903</td>\n",
       "      <td>0.055097</td>\n",
       "      <td>0.075262</td>\n",
       "      <td>0.055256</td>\n",
       "      <td>0.203715</td>\n",
       "      <td>0.061607</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.164020</td>\n",
       "      <td>0.001905</td>\n",
       "      <td>0.186408</td>\n",
       "      <td>0.026199</td>\n",
       "      <td>0.010003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.023024</td>\n",
       "      <td>0.008826</td>\n",
       "      <td>0.088642</td>\n",
       "      <td>0.007675</td>\n",
       "      <td>0.485418</td>\n",
       "      <td>0.213354</td>\n",
       "      <td>0.073676</td>\n",
       "      <td>0.040675</td>\n",
       "      <td>0.007291</td>\n",
       "      <td>0.012279</td>\n",
       "      <td>0.034152</td>\n",
       "      <td>0.004988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.090641</td>\n",
       "      <td>0.136117</td>\n",
       "      <td>0.217927</td>\n",
       "      <td>0.036024</td>\n",
       "      <td>0.168888</td>\n",
       "      <td>0.109622</td>\n",
       "      <td>0.064689</td>\n",
       "      <td>0.034785</td>\n",
       "      <td>0.022777</td>\n",
       "      <td>0.031608</td>\n",
       "      <td>0.081655</td>\n",
       "      <td>0.005268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.177578</td>\n",
       "      <td>0.013138</td>\n",
       "      <td>0.029049</td>\n",
       "      <td>0.004452</td>\n",
       "      <td>0.146595</td>\n",
       "      <td>0.263375</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.240530</td>\n",
       "      <td>0.009342</td>\n",
       "      <td>0.043938</td>\n",
       "      <td>0.017189</td>\n",
       "      <td>0.042588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.078035</td>\n",
       "      <td>0.005093</td>\n",
       "      <td>0.021028</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.012321</td>\n",
       "      <td>0.698538</td>\n",
       "      <td>0.066535</td>\n",
       "      <td>0.064564</td>\n",
       "      <td>0.020864</td>\n",
       "      <td>0.011171</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.016593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.090550</td>\n",
       "      <td>0.174729</td>\n",
       "      <td>0.027282</td>\n",
       "      <td>0.065961</td>\n",
       "      <td>0.088666</td>\n",
       "      <td>0.221215</td>\n",
       "      <td>0.044333</td>\n",
       "      <td>0.092973</td>\n",
       "      <td>0.080409</td>\n",
       "      <td>0.002423</td>\n",
       "      <td>0.052051</td>\n",
       "      <td>0.059320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.034967</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.212791</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.018530</td>\n",
       "      <td>0.352361</td>\n",
       "      <td>0.034668</td>\n",
       "      <td>0.117155</td>\n",
       "      <td>0.181112</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.002989</td>\n",
       "      <td>0.014345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.020143</td>\n",
       "      <td>0.103964</td>\n",
       "      <td>0.013970</td>\n",
       "      <td>0.017869</td>\n",
       "      <td>0.402209</td>\n",
       "      <td>0.245939</td>\n",
       "      <td>0.084146</td>\n",
       "      <td>0.041910</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.010071</td>\n",
       "      <td>0.002274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.119536</td>\n",
       "      <td>0.067550</td>\n",
       "      <td>0.022517</td>\n",
       "      <td>0.014901</td>\n",
       "      <td>0.348344</td>\n",
       "      <td>0.030132</td>\n",
       "      <td>0.127815</td>\n",
       "      <td>0.138742</td>\n",
       "      <td>0.030795</td>\n",
       "      <td>0.014570</td>\n",
       "      <td>0.078146</td>\n",
       "      <td>0.006954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.052753</td>\n",
       "      <td>0.122628</td>\n",
       "      <td>0.008329</td>\n",
       "      <td>0.056455</td>\n",
       "      <td>0.158723</td>\n",
       "      <td>0.346599</td>\n",
       "      <td>0.114762</td>\n",
       "      <td>0.035632</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.055530</td>\n",
       "      <td>0.000463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ADP       DET         X      PRON      VERB      NOUN       ADJ  \\\n",
       "ADP   0.017067  0.325120  0.035413  0.069867  0.008427  0.320427  0.105280   \n",
       "DET   0.009572  0.005384  0.045346  0.003709  0.039005  0.637593  0.205671   \n",
       "X     0.142903  0.055097  0.075262  0.055256  0.203715  0.061607  0.017625   \n",
       "PRON  0.023024  0.008826  0.088642  0.007675  0.485418  0.213354  0.073676   \n",
       "VERB  0.090641  0.136117  0.217927  0.036024  0.168888  0.109622  0.064689   \n",
       "NOUN  0.177578  0.013138  0.029049  0.004452  0.146595  0.263375  0.012225   \n",
       "ADJ   0.078035  0.005093  0.021028  0.000493  0.012321  0.698538  0.066535   \n",
       ".     0.090550  0.174729  0.027282  0.065961  0.088666  0.221215  0.044333   \n",
       "NUM   0.034967  0.003288  0.212791  0.001494  0.018530  0.352361  0.034668   \n",
       "PRT   0.020143  0.103964  0.013970  0.017869  0.402209  0.245939  0.084146   \n",
       "ADV   0.119536  0.067550  0.022517  0.014901  0.348344  0.030132  0.127815   \n",
       "CONJ  0.052753  0.122628  0.008329  0.056455  0.158723  0.346599  0.114762   \n",
       "\n",
       "             .       NUM       PRT       ADV      CONJ  \n",
       "ADP   0.039467  0.063040  0.001493  0.013653  0.000747  \n",
       "DET   0.017708  0.022493  0.000239  0.012802  0.000479  \n",
       "X     0.164020  0.001905  0.186408  0.026199  0.010003  \n",
       "PRON  0.040675  0.007291  0.012279  0.034152  0.004988  \n",
       "VERB  0.034785  0.022777  0.031608  0.081655  0.005268  \n",
       "NOUN  0.240530  0.009342  0.043938  0.017189  0.042588  \n",
       "ADJ   0.064564  0.020864  0.011171  0.004764  0.016593  \n",
       ".     0.092973  0.080409  0.002423  0.052051  0.059320  \n",
       "NUM   0.117155  0.181112  0.026300  0.002989  0.014345  \n",
       "PRT   0.041910  0.055556  0.001949  0.010071  0.002274  \n",
       "ADV   0.138742  0.030795  0.014570  0.078146  0.006954  \n",
       "CONJ  0.035632  0.043036  0.005090  0.055530  0.000463  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def vanilla_Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n",
      "264\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluating on Validation Set for 5 Sentences in validation set as it was taking \n",
    "more than 1 hr to run for entire validation set adn we would have to do it 3 times \n",
    "\"\"\"\n",
    "random.seed(1234)\n",
    "\n",
    "# choose random 10 sents\n",
    "rndom = [random.randint(1,len(valid_set)) for x in range(10)]\n",
    "\n",
    "# list of sents\n",
    "valid_run = [valid_set[i] for i in rndom]\n",
    "\n",
    "#list of tuples - validation taged words\n",
    "valid_tagged_words = [tup for sent in valid_run for tup in sent]\n",
    "print(len(valid_tagged_words))\n",
    "\n",
    "#list of words in validation set\n",
    "valid_words = [tup[0] for sent in valid_run for tup in sent]\n",
    "print(len(valid_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tagging the validation words using vanilla viterbi\n",
    "\n",
    "start = time.time()\n",
    "vanilla_valid_tagged_seq = vanilla_Viterbi(valid_words)\n",
    "end = time.time()\n",
    "difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  62.845826625823975\n",
      "[('*-1', 'X'), ('To', 'PRT'), ('offset', 'VERB'), ('the', 'DET'), ('reduction', 'NOUN'), (',', '.'), ('Congress', 'NOUN'), ('approved', 'VERB'), ('a', 'DET'), ('$', '.'), ('20,000', 'NUM'), ('*U*', 'X'), ('fee', 'NOUN'), ('that', 'ADP'), ('investors', 'NOUN'), ('and', 'CONJ'), ('companies', 'NOUN'), ('will', 'VERB'), ('have', 'VERB'), ('*-3', 'X'), ('to', 'PRT'), ('pay', 'VERB'), ('*T*-2', 'X'), ('each', 'DET'), ('time', 'NOUN'), ('0', 'X'), ('they', 'PRON'), ('make', 'VERB'), ('required', 'VERB'), ('filings', 'NOUN'), ('to', 'PRT'), ('antitrust', 'ADJ'), ('regulators', 'NOUN'), ('about', 'ADP'), ('mergers', 'NOUN'), (',', '.'), ('acquisitions', 'NOUN'), ('and', 'CONJ'), ('certain', 'ADJ'), ('other', 'ADJ'), ('transactions', 'NOUN'), ('*T*-4', 'X'), ('.', '.'), ('Volume', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('second', 'ADJ'), ('section', 'NOUN'), ('was', 'VERB'), ('estimated', 'VERB'), ('*-1', 'X'), ('at', 'ADP'), ('18', 'NUM'), ('million', 'NUM'), ('shares', 'NOUN'), (',', '.'), ('up', 'ADV'), ('from', 'ADP'), ('14', 'NUM'), ('million', 'NUM'), ('Tuesday', 'NOUN'), ('.', '.'), ('In', 'ADP'), ('less', 'ADJ'), ('parched', 'ADP'), ('areas', 'NOUN'), (',', '.'), ('meanwhile', 'ADV'), (',', '.'), ('farmers', 'NOUN'), ('who', 'PRON'), ('*T*-43', 'X'), ('had', 'VERB'), ('little', 'ADV'), ('or', 'CONJ'), ('no', 'DET'), ('loss', 'NOUN'), ('of', 'ADP'), ('production', 'NOUN'), ('profited', 'ADP'), ('greatly', 'ADV'), ('from', 'ADP'), ('the', 'DET'), ('higher', 'ADJ'), ('prices', 'NOUN'), ('.', '.'), ('*-1', 'X'), ('Moving', 'ADP'), ('rapidly', 'ADV'), ('through', 'ADP'), ('school', 'NOUN'), (',', '.'), ('he', 'PRON'), ('graduated', 'ADP'), ('Phi', 'ADP'), ('Beta', 'ADP'), ('Kappa', 'ADP'), ('from', 'ADP'), ('the', 'DET'), ('University', 'NOUN'), ('of', 'ADP'), ('Kentucky', 'ADP'), ('at', 'ADP'), ('age', 'NOUN'), ('18', 'NUM'), (',', '.'), ('after', 'ADP'), ('*', 'X'), ('spending', 'NOUN'), ('only', 'ADV'), ('2', 'NUM'), ('1\\\\/2', 'NUM'), ('years', 'NOUN'), ('in', 'ADP'), ('college', 'NOUN'), ('.', '.'), ('They', 'PRON'), ('blamed', 'VERB'), ('increased', 'VERB'), ('demand', 'NOUN'), ('for', 'ADP'), ('dairy', 'ADP'), ('products', 'NOUN'), ('at', 'ADP'), ('a', 'DET'), ('time', 'NOUN'), ('of', 'ADP'), ('exceptionally', 'ADV'), ('high', 'ADJ'), ('U.S.', 'NOUN'), ('exports', 'NOUN'), ('of', 'ADP'), ('dry', 'ADJ'), ('milk', 'NOUN'), (',', '.'), ('coupled', 'VERB'), ('*', 'X'), ('with', 'ADP'), ('very', 'ADV'), ('low', 'ADJ'), ('import', 'NOUN'), ('quotas', 'NOUN'), ('.', '.'), ('Indeed', 'ADV'), (',', '.'), ('Judge', 'NOUN'), (\"O'Brien\", 'NOUN'), ('ruled', 'VERB'), ('that', 'ADP'), ('``', '.'), ('it', 'PRON'), ('*EXP*-1', 'X'), ('would', 'VERB'), ('be', 'VERB'), ('easy', 'ADJ'), ('*', 'X'), ('to', 'PRT'), ('conclude', 'VERB'), ('that', 'ADP'), ('the', 'DET'), ('USIA', 'NOUN'), (\"'s\", 'PRT'), ('position', 'NOUN'), ('is', 'VERB'), ('`', '.'), ('inappropriate', 'ADJ'), ('or', 'CONJ'), ('even', 'ADV'), ('stupid', 'ADJ'), (',', '.'), (\"'\", '.'), (\"''\", '.'), ('but', 'CONJ'), ('it', 'PRON'), (\"'s\", 'VERB'), ('the', 'DET'), ('law', 'NOUN'), ('.', '.'), ('The', 'DET'), ('futures', 'NOUN'), ('industry', 'NOUN'), ('is', 'VERB'), ('regulated', 'VERB'), ('*-134', 'ADP'), ('by', 'ADP'), ('the', 'DET'), ('Commodity', 'NOUN'), ('Futures', 'NOUN'), ('Trading', 'NOUN'), ('Commission', 'NOUN'), (',', '.'), ('which', 'DET'), ('*T*-218', 'ADP'), ('reports', 'NOUN'), ('to', 'PRT'), ('the', 'DET'), ('Agriculture', 'NOUN'), ('committees', 'ADP'), ('in', 'ADP'), ('both', 'DET'), ('houses', 'NOUN'), ('.', '.'), ('*', 'X'), ('Determining', 'ADP'), ('that', 'DET'), ('may', 'VERB'), ('enable', 'ADP'), ('them', 'PRON'), ('to', 'PRT'), ('develop', 'VERB'), ('better', 'ADV'), ('ways', 'NOUN'), ('0', 'X'), ('*', 'X'), ('to', 'PRT'), ('introduce', 'VERB'), ('the', 'DET'), ('needed', 'ADJ'), ('crystal-lattice', 'ADJ'), ('patterns', 'NOUN'), ('*T*-1', 'X'), ('.', '.'), ('Even', 'ADV'), ('if', 'ADP'), ('there', 'DET'), ('is', 'VERB'), ('consumer', 'NOUN'), ('resistance', 'NOUN'), ('at', 'ADP'), ('first', 'ADJ'), (',', '.'), ('a', 'DET'), ('wine', 'NOUN'), ('that', 'ADP'), ('*T*-167', 'ADP'), ('wins', 'ADP'), ('high', 'ADJ'), ('ratings', 'NOUN'), ('from', 'ADP'), ('the', 'DET'), ('critics', 'NOUN'), ('will', 'VERB'), ('eventually', 'ADV'), ('move', 'VERB'), ('.', '.'), ('He', 'PRON'), ('says', 'VERB'), ('0', 'X'), ('Campbell', 'NOUN'), ('was', 'VERB'), (\"n't\", 'ADV'), ('even', 'ADV'), ('contacted', 'VERB'), ('*-1', 'X'), ('by', 'ADP'), ('the', 'DET'), ('magazine', 'NOUN'), ('for', 'ADP'), ('the', 'DET'), ('opportunity', 'NOUN'), ('*', 'X'), ('to', 'PRT'), ('comment', 'VERB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken in seconds: \", difference)\n",
    "print(vanilla_valid_tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vanilla_valid_tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set using Vanilla Viterbi:  91.29 %\n"
     ]
    }
   ],
   "source": [
    "#calculating vanilla accuracy on validation set(10 sentence)\n",
    "\n",
    "vanilla_check = [i for i, j in zip(vanilla_valid_tagged_seq, valid_tagged_words) if i == j]\n",
    "vanilla_accuracy = len(vanilla_check)/len(vanilla_valid_tagged_seq)\n",
    "\n",
    "print(\"Accuracy on validation set using Vanilla Viterbi: \",round(vanilla_accuracy*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('*-1', 'X'),\n",
       " ('To', 'PRT'),\n",
       " ('offset', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('reduction', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Congress', 'NOUN'),\n",
       " ('approved', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('$', '.'),\n",
       " ('20,000', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('fee', 'NOUN'),\n",
       " ('that', 'ADP'),\n",
       " ('investors', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('companies', 'NOUN'),\n",
       " ('will', 'VERB'),\n",
       " ('have', 'VERB'),\n",
       " ('*-3', 'X'),\n",
       " ('to', 'PRT'),\n",
       " ('pay', 'VERB'),\n",
       " ('*T*-2', 'X'),\n",
       " ('each', 'DET'),\n",
       " ('time', 'NOUN'),\n",
       " ('0', 'X'),\n",
       " ('they', 'PRON'),\n",
       " ('make', 'VERB'),\n",
       " ('required', 'VERB'),\n",
       " ('filings', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('antitrust', 'ADJ'),\n",
       " ('regulators', 'NOUN'),\n",
       " ('about', 'ADP'),\n",
       " ('mergers', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('acquisitions', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('certain', 'ADJ'),\n",
       " ('other', 'ADJ'),\n",
       " ('transactions', 'NOUN'),\n",
       " ('*T*-4', 'X'),\n",
       " ('.', '.'),\n",
       " ('Volume', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('second', 'ADJ'),\n",
       " ('section', 'NOUN'),\n",
       " ('was', 'VERB'),\n",
       " ('estimated', 'VERB'),\n",
       " ('*-1', 'X'),\n",
       " ('at', 'ADP'),\n",
       " ('18', 'NUM'),\n",
       " ('million', 'NUM'),\n",
       " ('shares', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('up', 'ADV'),\n",
       " ('from', 'ADP'),\n",
       " ('14', 'NUM'),\n",
       " ('million', 'NUM'),\n",
       " ('Tuesday', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('In', 'ADP'),\n",
       " ('less', 'ADJ'),\n",
       " ('parched', 'VERB'),\n",
       " ('areas', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('meanwhile', 'ADV'),\n",
       " (',', '.'),\n",
       " ('farmers', 'NOUN'),\n",
       " ('who', 'PRON'),\n",
       " ('*T*-43', 'X'),\n",
       " ('had', 'VERB'),\n",
       " ('little', 'ADJ'),\n",
       " ('or', 'CONJ'),\n",
       " ('no', 'DET'),\n",
       " ('loss', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('production', 'NOUN'),\n",
       " ('profited', 'VERB'),\n",
       " ('greatly', 'ADV'),\n",
       " ('from', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('higher', 'ADJ'),\n",
       " ('prices', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('*-1', 'X'),\n",
       " ('Moving', 'VERB'),\n",
       " ('rapidly', 'ADV'),\n",
       " ('through', 'ADP'),\n",
       " ('school', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('he', 'PRON'),\n",
       " ('graduated', 'VERB'),\n",
       " ('Phi', 'NOUN'),\n",
       " ('Beta', 'NOUN'),\n",
       " ('Kappa', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('University', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('Kentucky', 'NOUN'),\n",
       " ('at', 'ADP'),\n",
       " ('age', 'NOUN'),\n",
       " ('18', 'NUM'),\n",
       " (',', '.'),\n",
       " ('after', 'ADP'),\n",
       " ('*', 'X'),\n",
       " ('spending', 'VERB'),\n",
       " ('only', 'ADV'),\n",
       " ('2', 'NUM'),\n",
       " ('1\\\\/2', 'NUM'),\n",
       " ('years', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('college', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('They', 'PRON'),\n",
       " ('blamed', 'VERB'),\n",
       " ('increased', 'VERB'),\n",
       " ('demand', 'NOUN'),\n",
       " ('for', 'ADP'),\n",
       " ('dairy', 'NOUN'),\n",
       " ('products', 'NOUN'),\n",
       " ('at', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('time', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('exceptionally', 'ADV'),\n",
       " ('high', 'ADJ'),\n",
       " ('U.S.', 'NOUN'),\n",
       " ('exports', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('dry', 'ADJ'),\n",
       " ('milk', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('coupled', 'VERB'),\n",
       " ('*', 'X'),\n",
       " ('with', 'ADP'),\n",
       " ('very', 'ADV'),\n",
       " ('low', 'ADJ'),\n",
       " ('import', 'NOUN'),\n",
       " ('quotas', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Indeed', 'ADV'),\n",
       " (',', '.'),\n",
       " ('Judge', 'NOUN'),\n",
       " (\"O'Brien\", 'NOUN'),\n",
       " ('ruled', 'VERB'),\n",
       " ('that', 'ADP'),\n",
       " ('``', '.'),\n",
       " ('it', 'PRON'),\n",
       " ('*EXP*-1', 'X'),\n",
       " ('would', 'VERB'),\n",
       " ('be', 'VERB'),\n",
       " ('easy', 'ADJ'),\n",
       " ('*', 'X'),\n",
       " ('to', 'PRT'),\n",
       " ('conclude', 'VERB'),\n",
       " ('that', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('USIA', 'NOUN'),\n",
       " (\"'s\", 'PRT'),\n",
       " ('position', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('`', '.'),\n",
       " ('inappropriate', 'ADJ'),\n",
       " ('or', 'CONJ'),\n",
       " ('even', 'ADV'),\n",
       " ('stupid', 'ADJ'),\n",
       " (',', '.'),\n",
       " (\"'\", '.'),\n",
       " (\"''\", '.'),\n",
       " ('but', 'CONJ'),\n",
       " ('it', 'PRON'),\n",
       " (\"'s\", 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('law', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DET'),\n",
       " ('futures', 'NOUN'),\n",
       " ('industry', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('regulated', 'VERB'),\n",
       " ('*-134', 'X'),\n",
       " ('by', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('Commodity', 'NOUN'),\n",
       " ('Futures', 'NOUN'),\n",
       " ('Trading', 'NOUN'),\n",
       " ('Commission', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('which', 'DET'),\n",
       " ('*T*-218', 'X'),\n",
       " ('reports', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('the', 'DET'),\n",
       " ('Agriculture', 'NOUN'),\n",
       " ('committees', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('both', 'DET'),\n",
       " ('houses', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('*', 'X'),\n",
       " ('Determining', 'VERB'),\n",
       " ('that', 'DET'),\n",
       " ('may', 'VERB'),\n",
       " ('enable', 'VERB'),\n",
       " ('them', 'PRON'),\n",
       " ('to', 'PRT'),\n",
       " ('develop', 'VERB'),\n",
       " ('better', 'ADJ'),\n",
       " ('ways', 'NOUN'),\n",
       " ('0', 'X'),\n",
       " ('*', 'X'),\n",
       " ('to', 'PRT'),\n",
       " ('introduce', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('needed', 'VERB'),\n",
       " ('crystal-lattice', 'NOUN'),\n",
       " ('patterns', 'NOUN'),\n",
       " ('*T*-1', 'X'),\n",
       " ('.', '.'),\n",
       " ('Even', 'ADV'),\n",
       " ('if', 'ADP'),\n",
       " ('there', 'DET'),\n",
       " ('is', 'VERB'),\n",
       " ('consumer', 'NOUN'),\n",
       " ('resistance', 'NOUN'),\n",
       " ('at', 'ADP'),\n",
       " ('first', 'ADJ'),\n",
       " (',', '.'),\n",
       " ('a', 'DET'),\n",
       " ('wine', 'NOUN'),\n",
       " ('that', 'DET'),\n",
       " ('*T*-167', 'X'),\n",
       " ('wins', 'VERB'),\n",
       " ('high', 'ADJ'),\n",
       " ('ratings', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('critics', 'NOUN'),\n",
       " ('will', 'VERB'),\n",
       " ('eventually', 'ADV'),\n",
       " ('move', 'VERB'),\n",
       " ('.', '.'),\n",
       " ('He', 'PRON'),\n",
       " ('says', 'VERB'),\n",
       " ('0', 'X'),\n",
       " ('Campbell', 'NOUN'),\n",
       " ('was', 'VERB'),\n",
       " (\"n't\", 'ADV'),\n",
       " ('even', 'ADV'),\n",
       " ('contacted', 'VERB'),\n",
       " ('*-1', 'X'),\n",
       " ('by', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('magazine', 'NOUN'),\n",
       " ('for', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('opportunity', 'NOUN'),\n",
       " ('*', 'X'),\n",
       " ('to', 'PRT'),\n",
       " ('comment', 'VERB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_incorrect_tagged = [[valid_tagged_words[i-1],j] for i, j in enumerate(zip(vanilla_valid_tagged_seq, valid_tagged_words)) if j[0]!=j[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('less', 'ADJ'), (('parched', 'ADP'), ('parched', 'VERB'))],\n",
       " [('had', 'VERB'), (('little', 'ADV'), ('little', 'ADJ'))],\n",
       " [('production', 'NOUN'), (('profited', 'ADP'), ('profited', 'VERB'))],\n",
       " [('*-1', 'X'), (('Moving', 'ADP'), ('Moving', 'VERB'))],\n",
       " [('he', 'PRON'), (('graduated', 'ADP'), ('graduated', 'VERB'))],\n",
       " [('graduated', 'VERB'), (('Phi', 'ADP'), ('Phi', 'NOUN'))],\n",
       " [('Phi', 'NOUN'), (('Beta', 'ADP'), ('Beta', 'NOUN'))],\n",
       " [('Beta', 'NOUN'), (('Kappa', 'ADP'), ('Kappa', 'NOUN'))],\n",
       " [('of', 'ADP'), (('Kentucky', 'ADP'), ('Kentucky', 'NOUN'))],\n",
       " [('*', 'X'), (('spending', 'NOUN'), ('spending', 'VERB'))],\n",
       " [('for', 'ADP'), (('dairy', 'ADP'), ('dairy', 'NOUN'))],\n",
       " [('regulated', 'VERB'), (('*-134', 'ADP'), ('*-134', 'X'))],\n",
       " [('which', 'DET'), (('*T*-218', 'ADP'), ('*T*-218', 'X'))],\n",
       " [('*T*-218', 'X'), (('reports', 'NOUN'), ('reports', 'VERB'))],\n",
       " [('Agriculture', 'NOUN'), (('committees', 'ADP'), ('committees', 'NOUN'))],\n",
       " [('*', 'X'), (('Determining', 'ADP'), ('Determining', 'VERB'))],\n",
       " [('may', 'VERB'), (('enable', 'ADP'), ('enable', 'VERB'))],\n",
       " [('develop', 'VERB'), (('better', 'ADV'), ('better', 'ADJ'))],\n",
       " [('the', 'DET'), (('needed', 'ADJ'), ('needed', 'VERB'))],\n",
       " [('needed', 'VERB'),\n",
       "  (('crystal-lattice', 'ADJ'), ('crystal-lattice', 'NOUN'))],\n",
       " [('wine', 'NOUN'), (('that', 'ADP'), ('that', 'DET'))],\n",
       " [('that', 'DET'), (('*T*-167', 'ADP'), ('*T*-167', 'X'))],\n",
       " [('*T*-167', 'X'), (('wins', 'ADP'), ('wins', 'VERB'))]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_incorrect_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "264\n",
      "0.06060606060606061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['parched',\n",
       " 'profited',\n",
       " 'Moving',\n",
       " 'graduated',\n",
       " 'Phi',\n",
       " 'Beta',\n",
       " 'Kappa',\n",
       " 'Kentucky',\n",
       " 'dairy',\n",
       " '*-134',\n",
       " '*T*-218',\n",
       " 'committees',\n",
       " 'Determining',\n",
       " 'enable',\n",
       " '*T*-167',\n",
       " 'wins']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see if there are unknown words in validation set \n",
    "\n",
    "unknown_valid_words =[]\n",
    "\n",
    "for word in valid_words:\n",
    "    if word not in train_tokens:\n",
    "        #if word not in unknown_valid_words:\n",
    "        unknown_valid_words.append(word)\n",
    "    else: pass\n",
    "    \n",
    "#l = len(unknown_words)\n",
    "#u = len(set(test_untagged_words))\n",
    "print(len(unknown_valid_words))\n",
    "#print(len(set(valid_words)))\n",
    "print(len(valid_words))\n",
    "print(len(unknown_valid_words)/len(valid_words))\n",
    "unknown_valid_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating model on Unseen Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Android is a mobile operating system developed by Google.\\n',\n",
       " 'Android has been the best-selling OS worldwide on smartphones since 2011 and on tablets since 2013.\\n',\n",
       " \"Google and Twitter made a deal in 2015 that gave Google access to Twitter's firehose.\\n\",\n",
       " 'Twitter is an online news and social networking service on which users post and interact with messages known as tweets.\\n',\n",
       " 'Before entering politics, Donald Trump was a domineering businessman and a television personality.\\n',\n",
       " 'The 2018 FIFA World Cup is the 21st FIFA World Cup, an international football tournament contested once every four years.\\n',\n",
       " 'This is the first World Cup to be held in Eastern Europe and the 11th time that it has been held in Europe.\\n',\n",
       " 'Show me the cheapest round trips from Dallas to Atlanta\\n',\n",
       " 'I would like to see flights from Denver to Philadelphia.\\n',\n",
       " 'Show me the price of the flights leaving Atlanta at about 3 in the afternoon and arriving in San Francisco.\\n',\n",
       " 'NASA invited social media users to experience the launch of ICESAT-2 Satellite.\\n']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent=[]\n",
    "f = open(\"sampleTestSentences.txt\",'r')\n",
    "f1 = f.readlines()\n",
    "for line in f1:\n",
    "    test_sent.append(line)\n",
    "f.close\n",
    "test_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_untagged_tokens =[]\n",
    "for sent in test_sent:\n",
    "    test_untagged_tokens.append(word_tokenize(sent))\n",
    "    \n",
    "test_untagged_words=list(word for i in test_untagged_tokens for word in i)\n",
    "\n",
    "#list of words in validation set\n",
    "#valid_words = [tup[0] for sent in valid_run for tup in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.999496936798096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Android', 'ADP'),\n",
       " ('is', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('mobile', 'ADJ'),\n",
       " ('operating', 'NOUN'),\n",
       " ('system', 'NOUN'),\n",
       " ('developed', 'VERB'),\n",
       " ('by', 'ADP'),\n",
       " ('Google', 'ADP'),\n",
       " ('.', '.'),\n",
       " ('Android', 'ADP'),\n",
       " ('has', 'VERB'),\n",
       " ('been', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('best-selling', 'ADJ'),\n",
       " ('OS', 'ADP'),\n",
       " ('worldwide', 'ADP'),\n",
       " ('on', 'ADP'),\n",
       " ('smartphones', 'ADP'),\n",
       " ('since', 'ADP'),\n",
       " ('2011', 'ADP'),\n",
       " ('and', 'CONJ'),\n",
       " ('on', 'ADP'),\n",
       " ('tablets', 'NOUN'),\n",
       " ('since', 'ADP'),\n",
       " ('2013', 'ADP'),\n",
       " ('.', '.'),\n",
       " ('Google', 'ADP'),\n",
       " ('and', 'CONJ'),\n",
       " ('Twitter', 'ADP'),\n",
       " ('made', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('deal', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('2015', 'ADP'),\n",
       " ('that', 'DET'),\n",
       " ('gave', 'VERB'),\n",
       " ('Google', 'ADP'),\n",
       " ('access', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Twitter', 'ADP'),\n",
       " (\"'s\", 'PRT'),\n",
       " ('firehose', 'ADP'),\n",
       " ('.', '.'),\n",
       " ('Twitter', 'ADP'),\n",
       " ('is', 'VERB'),\n",
       " ('an', 'DET'),\n",
       " ('online', 'ADP'),\n",
       " ('news', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('social', 'ADJ'),\n",
       " ('networking', 'NOUN'),\n",
       " ('service', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('which', 'DET'),\n",
       " ('users', 'NOUN'),\n",
       " ('post', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('interact', 'ADP'),\n",
       " ('with', 'ADP'),\n",
       " ('messages', 'ADP'),\n",
       " ('known', 'ADJ'),\n",
       " ('as', 'ADP'),\n",
       " ('tweets', 'ADP'),\n",
       " ('.', '.'),\n",
       " ('Before', 'ADP'),\n",
       " ('entering', 'VERB'),\n",
       " ('politics', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Donald', 'NOUN'),\n",
       " ('Trump', 'NOUN'),\n",
       " ('was', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('domineering', 'ADP'),\n",
       " ('businessman', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('a', 'DET'),\n",
       " ('television', 'NOUN'),\n",
       " ('personality', 'ADP'),\n",
       " ('.', '.'),\n",
       " ('The', 'DET'),\n",
       " ('2018', 'ADP'),\n",
       " ('FIFA', 'ADP'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup', 'ADP'),\n",
       " ('is', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('21st', 'ADP'),\n",
       " ('FIFA', 'ADP'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup', 'ADP'),\n",
       " (',', '.'),\n",
       " ('an', 'DET'),\n",
       " ('international', 'ADJ'),\n",
       " ('football', 'NOUN'),\n",
       " ('tournament', 'ADP'),\n",
       " ('contested', 'ADP'),\n",
       " ('once', 'ADV'),\n",
       " ('every', 'DET'),\n",
       " ('four', 'NUM'),\n",
       " ('years', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('This', 'DET'),\n",
       " ('is', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('first', 'ADJ'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup', 'ADP'),\n",
       " ('to', 'PRT'),\n",
       " ('be', 'VERB'),\n",
       " ('held', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('Eastern', 'NOUN'),\n",
       " ('Europe', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('the', 'DET'),\n",
       " ('11th', 'ADJ'),\n",
       " ('time', 'NOUN'),\n",
       " ('that', 'ADP'),\n",
       " ('it', 'PRON'),\n",
       " ('has', 'VERB'),\n",
       " ('been', 'VERB'),\n",
       " ('held', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('Europe', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Show', 'NOUN'),\n",
       " ('me', 'PRON'),\n",
       " ('the', 'DET'),\n",
       " ('cheapest', 'ADJ'),\n",
       " ('round', 'NOUN'),\n",
       " ('trips', 'ADP'),\n",
       " ('from', 'ADP'),\n",
       " ('Dallas', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Atlanta', 'NOUN'),\n",
       " ('I', 'PRON'),\n",
       " ('would', 'VERB'),\n",
       " ('like', 'ADP'),\n",
       " ('to', 'PRT'),\n",
       " ('see', 'VERB'),\n",
       " ('flights', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('Denver', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Philadelphia', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Show', 'NOUN'),\n",
       " ('me', 'PRON'),\n",
       " ('the', 'DET'),\n",
       " ('price', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('flights', 'NOUN'),\n",
       " ('leaving', 'VERB'),\n",
       " ('Atlanta', 'NOUN'),\n",
       " ('at', 'ADP'),\n",
       " ('about', 'ADP'),\n",
       " ('3', 'NUM'),\n",
       " ('in', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('afternoon', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('arriving', 'ADP'),\n",
       " ('in', 'ADP'),\n",
       " ('San', 'NOUN'),\n",
       " ('Francisco', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('NASA', 'ADP'),\n",
       " ('invited', 'ADP'),\n",
       " ('social', 'ADJ'),\n",
       " ('media', 'NOUN'),\n",
       " ('users', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('experience', 'NOUN'),\n",
       " ('the', 'DET'),\n",
       " ('launch', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('ICESAT-2', 'ADP'),\n",
       " ('Satellite', 'ADP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "test_vanilla_tagged_seq = vanilla_Viterbi(test_untagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "print(difference)\n",
    "test_vanilla_tagged_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "115\n",
      "0.24347826086956523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Android',\n",
       " 'Google',\n",
       " 'OS',\n",
       " 'worldwide',\n",
       " 'smartphones',\n",
       " '2011',\n",
       " '2013',\n",
       " 'Twitter',\n",
       " '2015',\n",
       " 'firehose',\n",
       " 'online',\n",
       " 'interact',\n",
       " 'messages',\n",
       " 'tweets',\n",
       " 'domineering',\n",
       " 'personality',\n",
       " '2018',\n",
       " 'FIFA',\n",
       " 'Cup',\n",
       " '21st',\n",
       " 'tournament',\n",
       " 'contested',\n",
       " 'trips',\n",
       " 'arriving',\n",
       " 'NASA',\n",
       " 'invited',\n",
       " 'ICESAT-2',\n",
       " 'Satellite']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# listing words from test set which were not in train set \n",
    "\n",
    "unknown_test_words =[]\n",
    "\n",
    "for word in test_untagged_words:\n",
    "    if word not in train_tokens:\n",
    "        if word not in unknown_test_words:\n",
    "            unknown_test_words.append(word)\n",
    "    else: pass\n",
    "    \n",
    "#l = len(unknown_words)\n",
    "#u = len(set(test_untagged_words))\n",
    "print(len(unknown_test_words))\n",
    "print(len(set(test_untagged_words)))\n",
    "print(len(unknown_test_words)/len(set(test_untagged_words)))\n",
    "unknown_test_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "- so there are 28 such unknown words in Test sentence file that were not present in train set\n",
    "- and all unknown words are tagged wrongly as the first tag in tagging list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using nltk.pos_tag tagger which is a perceptron tagger trained on nltk wall stree journal data to tag test sentences \n",
    "\n",
    "test_pos_tags = nltk.pos_tag(test_untagged_words, tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Android', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('mobile', 'ADJ'),\n",
       " ('operating', 'NOUN'),\n",
       " ('system', 'NOUN'),\n",
       " ('developed', 'VERB'),\n",
       " ('by', 'ADP'),\n",
       " ('Google', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Android', 'NOUN'),\n",
       " ('has', 'VERB'),\n",
       " ('been', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('best-selling', 'ADJ'),\n",
       " ('OS', 'NOUN'),\n",
       " ('worldwide', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('smartphones', 'NOUN'),\n",
       " ('since', 'ADP'),\n",
       " ('2011', 'NUM'),\n",
       " ('and', 'CONJ'),\n",
       " ('on', 'ADP'),\n",
       " ('tablets', 'NOUN'),\n",
       " ('since', 'ADP'),\n",
       " ('2013', 'NUM'),\n",
       " ('.', '.'),\n",
       " ('Google', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('Twitter', 'NOUN'),\n",
       " ('made', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('deal', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('2015', 'NUM'),\n",
       " ('that', 'DET'),\n",
       " ('gave', 'VERB'),\n",
       " ('Google', 'NOUN'),\n",
       " ('access', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Twitter', 'NOUN'),\n",
       " (\"'s\", 'PRT'),\n",
       " ('firehose', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Twitter', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('an', 'DET'),\n",
       " ('online', 'ADJ'),\n",
       " ('news', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('social', 'ADJ'),\n",
       " ('networking', 'NOUN'),\n",
       " ('service', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('which', 'DET'),\n",
       " ('users', 'NOUN'),\n",
       " ('post', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('interact', 'NOUN'),\n",
       " ('with', 'ADP'),\n",
       " ('messages', 'NOUN'),\n",
       " ('known', 'VERB'),\n",
       " ('as', 'ADP'),\n",
       " ('tweets', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Before', 'ADP'),\n",
       " ('entering', 'VERB'),\n",
       " ('politics', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Donald', 'NOUN'),\n",
       " ('Trump', 'NOUN'),\n",
       " ('was', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('domineering', 'ADJ'),\n",
       " ('businessman', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('a', 'DET'),\n",
       " ('television', 'NOUN'),\n",
       " ('personality', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DET'),\n",
       " ('2018', 'NUM'),\n",
       " ('FIFA', 'NOUN'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('21st', 'NUM'),\n",
       " ('FIFA', 'NOUN'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('an', 'DET'),\n",
       " ('international', 'ADJ'),\n",
       " ('football', 'NOUN'),\n",
       " ('tournament', 'NOUN'),\n",
       " ('contested', 'VERB'),\n",
       " ('once', 'ADV'),\n",
       " ('every', 'DET'),\n",
       " ('four', 'NUM'),\n",
       " ('years', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('This', 'DET'),\n",
       " ('is', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('first', 'ADJ'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('be', 'VERB'),\n",
       " ('held', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('Eastern', 'NOUN'),\n",
       " ('Europe', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('the', 'DET'),\n",
       " ('11th', 'NUM'),\n",
       " ('time', 'NOUN'),\n",
       " ('that', 'ADP'),\n",
       " ('it', 'PRON'),\n",
       " ('has', 'VERB'),\n",
       " ('been', 'VERB'),\n",
       " ('held', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('Europe', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Show', 'VERB'),\n",
       " ('me', 'PRON'),\n",
       " ('the', 'DET'),\n",
       " ('cheapest', 'ADJ'),\n",
       " ('round', 'NOUN'),\n",
       " ('trips', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('Dallas', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Atlanta', 'NOUN'),\n",
       " ('I', 'PRON'),\n",
       " ('would', 'VERB'),\n",
       " ('like', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('see', 'VERB'),\n",
       " ('flights', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('Denver', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Philadelphia', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Show', 'VERB'),\n",
       " ('me', 'PRON'),\n",
       " ('the', 'DET'),\n",
       " ('price', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('flights', 'NOUN'),\n",
       " ('leaving', 'VERB'),\n",
       " ('Atlanta', 'NOUN'),\n",
       " ('at', 'ADP'),\n",
       " ('about', 'ADV'),\n",
       " ('3', 'NUM'),\n",
       " ('in', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('afternoon', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('arriving', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('San', 'NOUN'),\n",
       " ('Francisco', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('NASA', 'NOUN'),\n",
       " ('invited', 'VERB'),\n",
       " ('social', 'ADJ'),\n",
       " ('media', 'NOUN'),\n",
       " ('users', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('experience', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('launch', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('ICESAT-2', 'NOUN'),\n",
       " ('Satellite', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set using Vanilla Viterbi:  76.24 %\n"
     ]
    }
   ],
   "source": [
    "#calculating vanilla accuracy on test set(10 sentence)\n",
    "\n",
    "test_vanilla_check = [i for i, j in zip(test_vanilla_tagged_seq, test_pos_tags) if i == j]\n",
    "test_vanilla_accuracy = len(test_vanilla_check)/len(test_vanilla_tagged_seq)\n",
    "\n",
    "print(\"Accuracy on test set using Vanilla Viterbi: \",round(test_vanilla_accuracy*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- clearly the loss in accuracy is due to presence of nearly 25% of unique unknown words(29) out of unique words(115) in test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve the problem of unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach - 1 # Lexicon and Rules Based Tagging of Unknown Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a rule based patterns\n",
    "## we have seen in incorrectly tagged words in validation set that many 'ing' ending words should be verb\n",
    "## any number should be labelled as number \n",
    "## 'ed' ending should be tagged as Adv\n",
    "## any other scenario should be tagged as Noun\n",
    "\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VERB'),              # gerund\n",
    "    (r'.*ed$', 'VERB'),              # past tense\n",
    "    (r'[0-9]{1,}', 'NUM'),            # cardinal numbers\n",
    "    (r'.*', 'NOUN')                    # nouns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule based tagger\n",
    "rule_based_tagger = nltk.RegexpTagger(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexicon backed up by the rule-based tagger\n",
    "lexicon_tagger = nltk.UnigramTagger(train_set, backoff=rule_based_tagger,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram tagger\n",
    "bigram_tagger = nltk.BigramTagger(train_set, backoff=lexicon_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9507560277891296"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger.evaluate(valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Android', 'NOUN')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger.tag(['Android'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_rules_tagger(word):\n",
    "    #token = word_tokenize(word)\n",
    "    state = bigram_tagger.tag(token)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'Noun')]\n",
      "Noun\n"
     ]
    }
   ],
   "source": [
    "state = []\n",
    "state.append(unknown_rules_tagger('Android'))\n",
    "#abc = unknown_rules_tagger('Android')\n",
    "print(abc)\n",
    "print(abc[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADP', 'DET', 'X', 'PRON', 'VERB', 'NOUN', 'ADJ', '.', 'NUM', 'PRT', 'ADV', 'CONJ']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfor key, word in enumerate(words):\\n    #print(key, word)\\n    p=[]\\n    for tag in T:\\n        if key == 0:\\n            transition_p = tags_df.loc['.', tag]\\n        else:\\n            transition_p = tags_df.loc[state[-1], tag]      \\n        emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\\n        state_probability = emission_p *transition_p\\n        p.append(state_probability)\\n    print(p)\\n\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = valid_words\n",
    "state = []\n",
    "T = list(set([pair[1] for pair in train_tagged_words]))\n",
    "#print(T)\n",
    "for key, word in enumerate(words):\n",
    "    #print(key, word)\n",
    "    p=[]\n",
    "    for tag in T:\n",
    "        if key == 0:\n",
    "            transition_p = tags_df.loc['.', tag]\n",
    "        else:\n",
    "            transition_p = tags_df.loc[state[-1], tag]      \n",
    "        emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "        state_probability = emission_p *transition_p\n",
    "        p.append(state_probability)\n",
    "    print(p)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('In', 'ADP'), ('August', 'NOUN')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged_words[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified Viterbi Model for handling unknown words\n",
    "def rules_Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = []\n",
    "        #p1 = []\n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p *transition_p\n",
    "            p.append(state_probability)\n",
    "            #if emission_p == 0:\n",
    "                #state_probability_u = transition_p\n",
    "                #p1.append(state_probability_u)\n",
    "                               \n",
    "        #set_state_probability = set(p)\n",
    "        #set_state_probability.remove(0.0)\n",
    "        if sum(p)>0:\n",
    "            pmax = max(p) # getting state for which probability is maximum\n",
    "            state_max = T[p.index(pmax)]\n",
    "            state.append(state_max)\n",
    "            #print(state)\n",
    "        else:\n",
    "            #pmax1 = max(p1)            # getting state for which probability is maximum\n",
    "            #state_max = T[p1.index(pmax1)] \n",
    "            state.append(bigram_tagger.tag([word])[0][1])\n",
    "            \n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  78.38528609275818\n",
      "[('*-1', 'X'), ('To', 'PRT'), ('offset', 'VERB'), ('the', 'DET'), ('reduction', 'NOUN'), (',', '.'), ('Congress', 'NOUN'), ('approved', 'VERB'), ('a', 'DET'), ('$', '.'), ('20,000', 'NUM'), ('*U*', 'X'), ('fee', 'NOUN'), ('that', 'ADP'), ('investors', 'NOUN'), ('and', 'CONJ'), ('companies', 'NOUN'), ('will', 'VERB'), ('have', 'VERB'), ('*-3', 'X'), ('to', 'PRT'), ('pay', 'VERB'), ('*T*-2', 'X'), ('each', 'DET'), ('time', 'NOUN'), ('0', 'X'), ('they', 'PRON'), ('make', 'VERB'), ('required', 'VERB'), ('filings', 'NOUN'), ('to', 'PRT'), ('antitrust', 'ADJ'), ('regulators', 'NOUN'), ('about', 'ADP'), ('mergers', 'NOUN'), (',', '.'), ('acquisitions', 'NOUN'), ('and', 'CONJ'), ('certain', 'ADJ'), ('other', 'ADJ'), ('transactions', 'NOUN'), ('*T*-4', 'X'), ('.', '.'), ('Volume', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('second', 'ADJ'), ('section', 'NOUN'), ('was', 'VERB'), ('estimated', 'VERB'), ('*-1', 'X'), ('at', 'ADP'), ('18', 'NUM'), ('million', 'NUM'), ('shares', 'NOUN'), (',', '.'), ('up', 'ADV'), ('from', 'ADP'), ('14', 'NUM'), ('million', 'NUM'), ('Tuesday', 'NOUN'), ('.', '.'), ('In', 'ADP'), ('less', 'ADJ'), ('parched', 'VERB'), ('areas', 'NOUN'), (',', '.'), ('meanwhile', 'ADV'), (',', '.'), ('farmers', 'NOUN'), ('who', 'PRON'), ('*T*-43', 'X'), ('had', 'VERB'), ('little', 'ADV'), ('or', 'CONJ'), ('no', 'DET'), ('loss', 'NOUN'), ('of', 'ADP'), ('production', 'NOUN'), ('profited', 'VERB'), ('greatly', 'ADV'), ('from', 'ADP'), ('the', 'DET'), ('higher', 'ADJ'), ('prices', 'NOUN'), ('.', '.'), ('*-1', 'X'), ('Moving', 'VERB'), ('rapidly', 'ADV'), ('through', 'ADP'), ('school', 'NOUN'), (',', '.'), ('he', 'PRON'), ('graduated', 'VERB'), ('Phi', 'NOUN'), ('Beta', 'NOUN'), ('Kappa', 'NOUN'), ('from', 'ADP'), ('the', 'DET'), ('University', 'NOUN'), ('of', 'ADP'), ('Kentucky', 'NOUN'), ('at', 'ADP'), ('age', 'NOUN'), ('18', 'NUM'), (',', '.'), ('after', 'ADP'), ('*', 'X'), ('spending', 'NOUN'), ('only', 'ADV'), ('2', 'NUM'), ('1\\\\/2', 'NUM'), ('years', 'NOUN'), ('in', 'ADP'), ('college', 'NOUN'), ('.', '.'), ('They', 'PRON'), ('blamed', 'VERB'), ('increased', 'VERB'), ('demand', 'NOUN'), ('for', 'ADP'), ('dairy', 'NOUN'), ('products', 'NOUN'), ('at', 'ADP'), ('a', 'DET'), ('time', 'NOUN'), ('of', 'ADP'), ('exceptionally', 'ADV'), ('high', 'ADJ'), ('U.S.', 'NOUN'), ('exports', 'NOUN'), ('of', 'ADP'), ('dry', 'ADJ'), ('milk', 'NOUN'), (',', '.'), ('coupled', 'VERB'), ('*', 'X'), ('with', 'ADP'), ('very', 'ADV'), ('low', 'ADJ'), ('import', 'NOUN'), ('quotas', 'NOUN'), ('.', '.'), ('Indeed', 'ADV'), (',', '.'), ('Judge', 'NOUN'), (\"O'Brien\", 'NOUN'), ('ruled', 'VERB'), ('that', 'ADP'), ('``', '.'), ('it', 'PRON'), ('*EXP*-1', 'X'), ('would', 'VERB'), ('be', 'VERB'), ('easy', 'ADJ'), ('*', 'X'), ('to', 'PRT'), ('conclude', 'VERB'), ('that', 'ADP'), ('the', 'DET'), ('USIA', 'NOUN'), (\"'s\", 'PRT'), ('position', 'NOUN'), ('is', 'VERB'), ('`', '.'), ('inappropriate', 'ADJ'), ('or', 'CONJ'), ('even', 'ADV'), ('stupid', 'ADJ'), (',', '.'), (\"'\", '.'), (\"''\", '.'), ('but', 'CONJ'), ('it', 'PRON'), (\"'s\", 'VERB'), ('the', 'DET'), ('law', 'NOUN'), ('.', '.'), ('The', 'DET'), ('futures', 'NOUN'), ('industry', 'NOUN'), ('is', 'VERB'), ('regulated', 'VERB'), ('*-134', 'NOUN'), ('by', 'ADP'), ('the', 'DET'), ('Commodity', 'NOUN'), ('Futures', 'NOUN'), ('Trading', 'NOUN'), ('Commission', 'NOUN'), (',', '.'), ('which', 'DET'), ('*T*-218', 'NOUN'), ('reports', 'NOUN'), ('to', 'PRT'), ('the', 'DET'), ('Agriculture', 'NOUN'), ('committees', 'NOUN'), ('in', 'ADP'), ('both', 'DET'), ('houses', 'NOUN'), ('.', '.'), ('*', 'X'), ('Determining', 'VERB'), ('that', 'ADP'), ('may', 'VERB'), ('enable', 'NOUN'), ('them', 'PRON'), ('to', 'PRT'), ('develop', 'VERB'), ('better', 'ADV'), ('ways', 'NOUN'), ('0', 'X'), ('*', 'X'), ('to', 'PRT'), ('introduce', 'VERB'), ('the', 'DET'), ('needed', 'ADJ'), ('crystal-lattice', 'ADJ'), ('patterns', 'NOUN'), ('*T*-1', 'X'), ('.', '.'), ('Even', 'ADV'), ('if', 'ADP'), ('there', 'DET'), ('is', 'VERB'), ('consumer', 'NOUN'), ('resistance', 'NOUN'), ('at', 'ADP'), ('first', 'ADJ'), (',', '.'), ('a', 'DET'), ('wine', 'NOUN'), ('that', 'ADP'), ('*T*-167', 'NOUN'), ('wins', 'NOUN'), ('high', 'ADJ'), ('ratings', 'NOUN'), ('from', 'ADP'), ('the', 'DET'), ('critics', 'NOUN'), ('will', 'VERB'), ('eventually', 'ADV'), ('move', 'VERB'), ('.', '.'), ('He', 'PRON'), ('says', 'VERB'), ('0', 'X'), ('Campbell', 'NOUN'), ('was', 'VERB'), (\"n't\", 'ADV'), ('even', 'ADV'), ('contacted', 'VERB'), ('*-1', 'X'), ('by', 'ADP'), ('the', 'DET'), ('magazine', 'NOUN'), ('for', 'ADP'), ('the', 'DET'), ('opportunity', 'NOUN'), ('*', 'X'), ('to', 'PRT'), ('comment', 'VERB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#tagging the validation words using rules viterbi\n",
    "\n",
    "start = time.time()\n",
    "rules_valid_tagged_seq = rules_Viterbi(valid_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "print(\"Time taken in seconds: \", difference)\n",
    "print(rules_valid_tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rules_valid_tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set using prob Viterbi:  95.08 %\n"
     ]
    }
   ],
   "source": [
    "#calculating rules accuracy on validation set(10 sentence)\n",
    "\n",
    "rules_check = [i for i, j in zip(rules_valid_tagged_seq, valid_tagged_words) if i == j]\n",
    "rules_accuracy = len(rules_check)/len(rules_valid_tagged_seq)\n",
    "\n",
    "print(\"Accuracy on validation set using prob Viterbi: \",round(rules_accuracy*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.66537666320801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Android', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('mobile', 'ADJ'),\n",
       " ('operating', 'NOUN'),\n",
       " ('system', 'NOUN'),\n",
       " ('developed', 'VERB'),\n",
       " ('by', 'ADP'),\n",
       " ('Google', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Android', 'NOUN'),\n",
       " ('has', 'VERB'),\n",
       " ('been', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('best-selling', 'ADJ'),\n",
       " ('OS', 'NOUN'),\n",
       " ('worldwide', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('smartphones', 'NOUN'),\n",
       " ('since', 'ADP'),\n",
       " ('2011', 'NUM'),\n",
       " ('and', 'CONJ'),\n",
       " ('on', 'ADP'),\n",
       " ('tablets', 'NOUN'),\n",
       " ('since', 'ADP'),\n",
       " ('2013', 'NUM'),\n",
       " ('.', '.'),\n",
       " ('Google', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('Twitter', 'NOUN'),\n",
       " ('made', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('deal', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('2015', 'NUM'),\n",
       " ('that', 'ADP'),\n",
       " ('gave', 'VERB'),\n",
       " ('Google', 'NOUN'),\n",
       " ('access', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Twitter', 'NOUN'),\n",
       " (\"'s\", 'PRT'),\n",
       " ('firehose', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Twitter', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('an', 'DET'),\n",
       " ('online', 'NOUN'),\n",
       " ('news', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('social', 'ADJ'),\n",
       " ('networking', 'NOUN'),\n",
       " ('service', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('which', 'DET'),\n",
       " ('users', 'NOUN'),\n",
       " ('post', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('interact', 'NOUN'),\n",
       " ('with', 'ADP'),\n",
       " ('messages', 'NOUN'),\n",
       " ('known', 'VERB'),\n",
       " ('as', 'ADP'),\n",
       " ('tweets', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Before', 'ADP'),\n",
       " ('entering', 'VERB'),\n",
       " ('politics', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Donald', 'NOUN'),\n",
       " ('Trump', 'NOUN'),\n",
       " ('was', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('domineering', 'VERB'),\n",
       " ('businessman', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('a', 'DET'),\n",
       " ('television', 'NOUN'),\n",
       " ('personality', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DET'),\n",
       " ('2018', 'NUM'),\n",
       " ('FIFA', 'NOUN'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('21st', 'NUM'),\n",
       " ('FIFA', 'NOUN'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('an', 'DET'),\n",
       " ('international', 'ADJ'),\n",
       " ('football', 'NOUN'),\n",
       " ('tournament', 'NOUN'),\n",
       " ('contested', 'VERB'),\n",
       " ('once', 'ADV'),\n",
       " ('every', 'DET'),\n",
       " ('four', 'NUM'),\n",
       " ('years', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('This', 'DET'),\n",
       " ('is', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('first', 'ADJ'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('be', 'VERB'),\n",
       " ('held', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('Eastern', 'NOUN'),\n",
       " ('Europe', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('the', 'DET'),\n",
       " ('11th', 'ADJ'),\n",
       " ('time', 'NOUN'),\n",
       " ('that', 'ADP'),\n",
       " ('it', 'PRON'),\n",
       " ('has', 'VERB'),\n",
       " ('been', 'VERB'),\n",
       " ('held', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('Europe', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Show', 'NOUN'),\n",
       " ('me', 'PRON'),\n",
       " ('the', 'DET'),\n",
       " ('cheapest', 'ADJ'),\n",
       " ('round', 'NOUN'),\n",
       " ('trips', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('Dallas', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Atlanta', 'NOUN'),\n",
       " ('I', 'PRON'),\n",
       " ('would', 'VERB'),\n",
       " ('like', 'ADP'),\n",
       " ('to', 'PRT'),\n",
       " ('see', 'VERB'),\n",
       " ('flights', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('Denver', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Philadelphia', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Show', 'NOUN'),\n",
       " ('me', 'PRON'),\n",
       " ('the', 'DET'),\n",
       " ('price', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('flights', 'NOUN'),\n",
       " ('leaving', 'VERB'),\n",
       " ('Atlanta', 'NOUN'),\n",
       " ('at', 'ADP'),\n",
       " ('about', 'ADP'),\n",
       " ('3', 'NUM'),\n",
       " ('in', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('afternoon', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('arriving', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('San', 'NOUN'),\n",
       " ('Francisco', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('NASA', 'NOUN'),\n",
       " ('invited', 'VERB'),\n",
       " ('social', 'ADJ'),\n",
       " ('media', 'NOUN'),\n",
       " ('users', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('experience', 'NOUN'),\n",
       " ('the', 'DET'),\n",
       " ('launch', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('ICESAT-2', 'NOUN'),\n",
       " ('Satellite', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "test_rules_tagged_seq = rules_Viterbi(test_untagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "print(difference)\n",
    "test_rules_tagged_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set using prob Viterbi:  94.48 %\n"
     ]
    }
   ],
   "source": [
    "#calculating vanilla accuracy on validation set(10 sentence)\n",
    "\n",
    "test_rules_check = [i for i, j in zip(test_rules_tagged_seq, test_pos_tags) if i == j]\n",
    "test_rules_accuracy = len(test_rules_check)/len(test_rules_tagged_seq)\n",
    "\n",
    "print(\"Accuracy on validation set using prob Viterbi: \",round(test_rules_accuracy*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach - 2 # Probabilistic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified Viterbi Model for handling unknown words\n",
    "def prob2_Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = []\n",
    "        p1 = []\n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p *transition_p\n",
    "            p.append(state_probability)\n",
    "            if emission_p == 0:\n",
    "                state_probability_u = transition_p\n",
    "                p1.append(state_probability_u)\n",
    "                               \n",
    "        #set_state_probability = set(p)\n",
    "        #set_state_probability.remove(0.0)\n",
    "        if sum(p)>0:\n",
    "            pmax = max(p) # getting state for which probability is maximum\n",
    "            state_max = T[p.index(pmax)]\n",
    "            state.append(state_max)\n",
    "        else:\n",
    "            pmax1 = max(p1)            # getting state for which probability is maximum\n",
    "            state_max = T[p1.index(pmax1)] \n",
    "            state.append(state_max)\n",
    "            \n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  69.84058880805969\n",
      "[('*-1', 'X'), ('To', 'PRT'), ('offset', 'VERB'), ('the', 'DET'), ('reduction', 'NOUN'), (',', '.'), ('Congress', 'NOUN'), ('approved', 'VERB'), ('a', 'DET'), ('$', '.'), ('20,000', 'NUM'), ('*U*', 'X'), ('fee', 'NOUN'), ('that', 'ADP'), ('investors', 'NOUN'), ('and', 'CONJ'), ('companies', 'NOUN'), ('will', 'VERB'), ('have', 'VERB'), ('*-3', 'X'), ('to', 'PRT'), ('pay', 'VERB'), ('*T*-2', 'X'), ('each', 'DET'), ('time', 'NOUN'), ('0', 'X'), ('they', 'PRON'), ('make', 'VERB'), ('required', 'VERB'), ('filings', 'NOUN'), ('to', 'PRT'), ('antitrust', 'ADJ'), ('regulators', 'NOUN'), ('about', 'ADP'), ('mergers', 'NOUN'), (',', '.'), ('acquisitions', 'NOUN'), ('and', 'CONJ'), ('certain', 'ADJ'), ('other', 'ADJ'), ('transactions', 'NOUN'), ('*T*-4', 'X'), ('.', '.'), ('Volume', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('second', 'ADJ'), ('section', 'NOUN'), ('was', 'VERB'), ('estimated', 'VERB'), ('*-1', 'X'), ('at', 'ADP'), ('18', 'NUM'), ('million', 'NUM'), ('shares', 'NOUN'), (',', '.'), ('up', 'ADV'), ('from', 'ADP'), ('14', 'NUM'), ('million', 'NUM'), ('Tuesday', 'NOUN'), ('.', '.'), ('In', 'ADP'), ('less', 'ADJ'), ('parched', 'NOUN'), ('areas', 'NOUN'), (',', '.'), ('meanwhile', 'ADV'), (',', '.'), ('farmers', 'NOUN'), ('who', 'PRON'), ('*T*-43', 'X'), ('had', 'VERB'), ('little', 'ADV'), ('or', 'CONJ'), ('no', 'DET'), ('loss', 'NOUN'), ('of', 'ADP'), ('production', 'NOUN'), ('profited', 'NOUN'), ('greatly', 'ADV'), ('from', 'ADP'), ('the', 'DET'), ('higher', 'ADJ'), ('prices', 'NOUN'), ('.', '.'), ('*-1', 'X'), ('Moving', 'VERB'), ('rapidly', 'ADV'), ('through', 'ADP'), ('school', 'NOUN'), (',', '.'), ('he', 'PRON'), ('graduated', 'VERB'), ('Phi', 'X'), ('Beta', 'VERB'), ('Kappa', 'X'), ('from', 'ADP'), ('the', 'DET'), ('University', 'NOUN'), ('of', 'ADP'), ('Kentucky', 'DET'), ('at', 'ADP'), ('age', 'NOUN'), ('18', 'NUM'), (',', '.'), ('after', 'ADP'), ('*', 'X'), ('spending', 'NOUN'), ('only', 'ADV'), ('2', 'NUM'), ('1\\\\/2', 'NUM'), ('years', 'NOUN'), ('in', 'ADP'), ('college', 'NOUN'), ('.', '.'), ('They', 'PRON'), ('blamed', 'VERB'), ('increased', 'VERB'), ('demand', 'NOUN'), ('for', 'ADP'), ('dairy', 'DET'), ('products', 'NOUN'), ('at', 'ADP'), ('a', 'DET'), ('time', 'NOUN'), ('of', 'ADP'), ('exceptionally', 'ADV'), ('high', 'ADJ'), ('U.S.', 'NOUN'), ('exports', 'NOUN'), ('of', 'ADP'), ('dry', 'ADJ'), ('milk', 'NOUN'), (',', '.'), ('coupled', 'VERB'), ('*', 'X'), ('with', 'ADP'), ('very', 'ADV'), ('low', 'ADJ'), ('import', 'NOUN'), ('quotas', 'NOUN'), ('.', '.'), ('Indeed', 'ADV'), (',', '.'), ('Judge', 'NOUN'), (\"O'Brien\", 'NOUN'), ('ruled', 'VERB'), ('that', 'ADP'), ('``', '.'), ('it', 'PRON'), ('*EXP*-1', 'X'), ('would', 'VERB'), ('be', 'VERB'), ('easy', 'ADJ'), ('*', 'X'), ('to', 'PRT'), ('conclude', 'VERB'), ('that', 'ADP'), ('the', 'DET'), ('USIA', 'NOUN'), (\"'s\", 'PRT'), ('position', 'NOUN'), ('is', 'VERB'), ('`', '.'), ('inappropriate', 'ADJ'), ('or', 'CONJ'), ('even', 'ADV'), ('stupid', 'ADJ'), (',', '.'), (\"'\", '.'), (\"''\", '.'), ('but', 'CONJ'), ('it', 'PRON'), (\"'s\", 'VERB'), ('the', 'DET'), ('law', 'NOUN'), ('.', '.'), ('The', 'DET'), ('futures', 'NOUN'), ('industry', 'NOUN'), ('is', 'VERB'), ('regulated', 'VERB'), ('*-134', 'X'), ('by', 'ADP'), ('the', 'DET'), ('Commodity', 'NOUN'), ('Futures', 'NOUN'), ('Trading', 'NOUN'), ('Commission', 'NOUN'), (',', '.'), ('which', 'DET'), ('*T*-218', 'NOUN'), ('reports', 'NOUN'), ('to', 'PRT'), ('the', 'DET'), ('Agriculture', 'NOUN'), ('committees', 'NOUN'), ('in', 'ADP'), ('both', 'DET'), ('houses', 'NOUN'), ('.', '.'), ('*', 'X'), ('Determining', 'VERB'), ('that', 'ADP'), ('may', 'VERB'), ('enable', 'X'), ('them', 'PRON'), ('to', 'PRT'), ('develop', 'VERB'), ('better', 'ADV'), ('ways', 'NOUN'), ('0', 'X'), ('*', 'X'), ('to', 'PRT'), ('introduce', 'VERB'), ('the', 'DET'), ('needed', 'ADJ'), ('crystal-lattice', 'ADJ'), ('patterns', 'NOUN'), ('*T*-1', 'X'), ('.', '.'), ('Even', 'ADV'), ('if', 'ADP'), ('there', 'DET'), ('is', 'VERB'), ('consumer', 'NOUN'), ('resistance', 'NOUN'), ('at', 'ADP'), ('first', 'ADJ'), (',', '.'), ('a', 'DET'), ('wine', 'NOUN'), ('that', 'ADP'), ('*T*-167', 'DET'), ('wins', 'NOUN'), ('high', 'ADJ'), ('ratings', 'NOUN'), ('from', 'ADP'), ('the', 'DET'), ('critics', 'NOUN'), ('will', 'VERB'), ('eventually', 'ADV'), ('move', 'VERB'), ('.', '.'), ('He', 'PRON'), ('says', 'VERB'), ('0', 'X'), ('Campbell', 'NOUN'), ('was', 'VERB'), (\"n't\", 'ADV'), ('even', 'ADV'), ('contacted', 'VERB'), ('*-1', 'X'), ('by', 'ADP'), ('the', 'DET'), ('magazine', 'NOUN'), ('for', 'ADP'), ('the', 'DET'), ('opportunity', 'NOUN'), ('*', 'X'), ('to', 'PRT'), ('comment', 'VERB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#tagging the validation words using prob2 viterbi\n",
    "\n",
    "start = time.time()\n",
    "prob2_valid_tagged_seq = prob2_Viterbi(valid_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "print(\"Time taken in seconds: \", difference)\n",
    "print(prob2_valid_tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set using prob2 Viterbi:  92.8 %\n"
     ]
    }
   ],
   "source": [
    "#calculating prob2 accuracy on validation set(10 sentence)\n",
    "\n",
    "prob2_check = [i for i, j in zip(prob2_valid_tagged_seq, valid_tagged_words) if i == j]\n",
    "prob2_accuracy = len(prob2_check)/len(prob2_valid_tagged_seq)\n",
    "\n",
    "print(\"Accuracy on validation set using prob2 Viterbi: \",round(prob2_accuracy*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.11868357658386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Android', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('mobile', 'ADJ'),\n",
       " ('operating', 'NOUN'),\n",
       " ('system', 'NOUN'),\n",
       " ('developed', 'VERB'),\n",
       " ('by', 'ADP'),\n",
       " ('Google', 'DET'),\n",
       " ('.', '.'),\n",
       " ('Android', 'NOUN'),\n",
       " ('has', 'VERB'),\n",
       " ('been', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('best-selling', 'ADJ'),\n",
       " ('OS', 'NOUN'),\n",
       " ('worldwide', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('smartphones', 'DET'),\n",
       " ('since', 'ADP'),\n",
       " ('2011', 'DET'),\n",
       " ('and', 'CONJ'),\n",
       " ('on', 'ADP'),\n",
       " ('tablets', 'NOUN'),\n",
       " ('since', 'ADP'),\n",
       " ('2013', 'DET'),\n",
       " ('.', '.'),\n",
       " ('Google', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('Twitter', 'NOUN'),\n",
       " ('made', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('deal', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('2015', 'DET'),\n",
       " ('that', 'ADP'),\n",
       " ('gave', 'VERB'),\n",
       " ('Google', 'X'),\n",
       " ('access', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Twitter', 'VERB'),\n",
       " (\"'s\", 'PRT'),\n",
       " ('firehose', 'VERB'),\n",
       " ('.', '.'),\n",
       " ('Twitter', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('an', 'DET'),\n",
       " ('online', 'NOUN'),\n",
       " ('news', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('social', 'ADJ'),\n",
       " ('networking', 'NOUN'),\n",
       " ('service', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('which', 'DET'),\n",
       " ('users', 'NOUN'),\n",
       " ('post', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('interact', 'NOUN'),\n",
       " ('with', 'ADP'),\n",
       " ('messages', 'DET'),\n",
       " ('known', 'ADJ'),\n",
       " ('as', 'ADP'),\n",
       " ('tweets', 'DET'),\n",
       " ('.', '.'),\n",
       " ('Before', 'ADP'),\n",
       " ('entering', 'VERB'),\n",
       " ('politics', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Donald', 'NOUN'),\n",
       " ('Trump', 'NOUN'),\n",
       " ('was', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('domineering', 'NOUN'),\n",
       " ('businessman', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('a', 'DET'),\n",
       " ('television', 'NOUN'),\n",
       " ('personality', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DET'),\n",
       " ('2018', 'NOUN'),\n",
       " ('FIFA', 'NOUN'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('21st', 'NOUN'),\n",
       " ('FIFA', 'NOUN'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('an', 'DET'),\n",
       " ('international', 'ADJ'),\n",
       " ('football', 'NOUN'),\n",
       " ('tournament', 'NOUN'),\n",
       " ('contested', 'NOUN'),\n",
       " ('once', 'ADV'),\n",
       " ('every', 'DET'),\n",
       " ('four', 'NUM'),\n",
       " ('years', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('This', 'DET'),\n",
       " ('is', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('first', 'ADJ'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('be', 'VERB'),\n",
       " ('held', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('Eastern', 'NOUN'),\n",
       " ('Europe', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('the', 'DET'),\n",
       " ('11th', 'ADJ'),\n",
       " ('time', 'NOUN'),\n",
       " ('that', 'ADP'),\n",
       " ('it', 'PRON'),\n",
       " ('has', 'VERB'),\n",
       " ('been', 'VERB'),\n",
       " ('held', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('Europe', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Show', 'NOUN'),\n",
       " ('me', 'PRON'),\n",
       " ('the', 'DET'),\n",
       " ('cheapest', 'ADJ'),\n",
       " ('round', 'NOUN'),\n",
       " ('trips', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('Dallas', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Atlanta', 'NOUN'),\n",
       " ('I', 'PRON'),\n",
       " ('would', 'VERB'),\n",
       " ('like', 'ADP'),\n",
       " ('to', 'PRT'),\n",
       " ('see', 'VERB'),\n",
       " ('flights', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('Denver', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Philadelphia', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Show', 'NOUN'),\n",
       " ('me', 'PRON'),\n",
       " ('the', 'DET'),\n",
       " ('price', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('flights', 'NOUN'),\n",
       " ('leaving', 'VERB'),\n",
       " ('Atlanta', 'NOUN'),\n",
       " ('at', 'ADP'),\n",
       " ('about', 'ADP'),\n",
       " ('3', 'NUM'),\n",
       " ('in', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('afternoon', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('arriving', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('San', 'NOUN'),\n",
       " ('Francisco', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('NASA', 'NOUN'),\n",
       " ('invited', 'NOUN'),\n",
       " ('social', 'ADJ'),\n",
       " ('media', 'NOUN'),\n",
       " ('users', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('experience', 'NOUN'),\n",
       " ('the', 'DET'),\n",
       " ('launch', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('ICESAT-2', 'DET'),\n",
       " ('Satellite', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "test_prob2_tagged_seq = prob2_Viterbi(test_untagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "print(difference)\n",
    "test_prob2_tagged_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set using prob2 Viterbi:  86.19 %\n"
     ]
    }
   ],
   "source": [
    "#calculating vanilla accuracy on validation set(10 sentence)\n",
    "\n",
    "test_prob2_check = [i for i, j in zip(test_prob2_tagged_seq, test_pos_tags) if i == j]\n",
    "test_prob2_accuracy = len(test_prob2_check)/len(test_prob2_tagged_seq)\n",
    "\n",
    "print(\"Accuracy on validation set using prob2 Viterbi: \",round(test_prob2_accuracy*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating Tagging Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging Accuracy of Vanilla Viterbi on Validation Set:  91.29 %\n"
     ]
    }
   ],
   "source": [
    "# Tagging Accuracy of Vanilla Viterbi on Validation Set\n",
    "print(\"Tagging Accuracy of Vanilla Viterbi on Validation Set: \",round(vanilla_accuracy*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging Accuracy of Viterbi with Lexicon & Rules based on Validation Set:  95.08 %\n"
     ]
    }
   ],
   "source": [
    "# Tagging Accuracy of Viterbi Modification with Lexicon & Rules based approach on Validation Set\n",
    "print(\"Tagging Accuracy of Viterbi with Lexicon & Rules based on Validation Set: \",round(rules_accuracy*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set using prob2 Viterbi:  92.8 %\n"
     ]
    }
   ],
   "source": [
    "# Tagging Accuracy of Viterbi Modification with Probabilistic based approach on Validation Set\n",
    "print(\"Accuracy on validation set using prob2 Viterbi: \",round(prob2_accuracy*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set using Vanilla Viterbi:  76.24 %\n"
     ]
    }
   ],
   "source": [
    "# Tagging Accuracy of Vanilla Viterbi on Test Sentence File\n",
    "print(\"Accuracy on test set using Vanilla Viterbi: \",round(test_vanilla_accuracy*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set using prob Viterbi:  94.48 %\n"
     ]
    }
   ],
   "source": [
    "# Tagging Accuracy of Viterbi Modification with Lexicon & Rules based approach on Test Sentence File\n",
    "print(\"Accuracy on validation set using prob Viterbi: \",round(test_rules_accuracy*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set using prob2 Viterbi:  86.19 %\n"
     ]
    }
   ],
   "source": [
    "# Tagging Accuracy of Viterbi Modification with Probabilistic based approach on Validation Set\n",
    "print(\"Accuracy on validation set using prob2 Viterbi: \",round(test_prob2_accuracy*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "- As we can clearly see that by applying modifications on Vanilla Viterbi \n",
    "- Maximum tagging accuracy was achieved with a Lexicon and Rules based approach for treating unknown words in vanilla viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('domineering', 'ADJ'),\n",
       " ('worldwide', 'NOUN'),\n",
       " ('Cup', 'NOUN'),\n",
       " ('2013', 'NUM'),\n",
       " ('smartphones', 'NOUN'),\n",
       " ('2018', 'NUM'),\n",
       " ('trips', 'NOUN'),\n",
       " ('tweets', 'NOUN'),\n",
       " ('arriving', 'NOUN'),\n",
       " ('invited', 'VERB'),\n",
       " ('interact', 'NOUN'),\n",
       " ('firehose', 'NOUN'),\n",
       " ('online', 'ADJ'),\n",
       " ('FIFA', 'NOUN'),\n",
       " ('tournament', 'NOUN'),\n",
       " ('NASA', 'NOUN'),\n",
       " ('Satellite', 'NOUN'),\n",
       " ('personality', 'NOUN'),\n",
       " ('ICESAT-2', 'NOUN'),\n",
       " ('21st', 'NUM'),\n",
       " ('Android', 'NOUN'),\n",
       " ('OS', 'NOUN'),\n",
       " ('Twitter', 'NOUN'),\n",
       " ('contested', 'VERB'),\n",
       " ('messages', 'NOUN'),\n",
       " ('2011', 'NUM'),\n",
       " ('2015', 'NUM'),\n",
       " ('Google', 'NOUN')]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since we are not provided with Pos tags on Test Sentences, we use a nltk.pos_tag which is a perceptron tagger pretrained model on wallstreet journas corpora\n",
    "# let's see the POS tags on words in Test sentence file which were not present in Train Set or treebank \n",
    "\n",
    "actual_tag_unknown_word = [pair for word in unknown_test_words for pair in test_pos_tags if word == pair[0]]\n",
    "list(set(actual_tag_unknown_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Twitter', 'ADP'),\n",
       " ('interact', 'ADP'),\n",
       " ('2015', 'ADP'),\n",
       " ('trips', 'ADP'),\n",
       " ('online', 'ADP'),\n",
       " ('smartphones', 'ADP'),\n",
       " ('firehose', 'ADP'),\n",
       " ('messages', 'ADP'),\n",
       " ('contested', 'ADP'),\n",
       " ('worldwide', 'ADP'),\n",
       " ('arriving', 'ADP'),\n",
       " ('domineering', 'ADP'),\n",
       " ('NASA', 'ADP'),\n",
       " ('tournament', 'ADP'),\n",
       " ('Satellite', 'ADP'),\n",
       " ('2011', 'ADP'),\n",
       " ('2018', 'ADP'),\n",
       " ('2013', 'ADP'),\n",
       " ('ICESAT-2', 'ADP'),\n",
       " ('personality', 'ADP'),\n",
       " ('tweets', 'ADP'),\n",
       " ('invited', 'ADP'),\n",
       " ('OS', 'ADP'),\n",
       " ('FIFA', 'ADP'),\n",
       " ('Google', 'ADP'),\n",
       " ('Android', 'ADP'),\n",
       " ('21st', 'ADP'),\n",
       " ('Cup', 'ADP')]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see the POS tags generated by Vanilla Viterbi on Uknown words from test Sentence file (not present in treebank train set) \n",
    "\n",
    "vanilla_tag_unknown_word = [pair for word in unknown_test_words for pair in test_vanilla_tagged_seq if word == pair[0]]\n",
    "list(set(vanilla_tag_unknown_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worldwide', 'NOUN'),\n",
       " ('Cup', 'NOUN'),\n",
       " ('2013', 'NUM'),\n",
       " ('smartphones', 'NOUN'),\n",
       " ('online', 'NOUN'),\n",
       " ('2018', 'NUM'),\n",
       " ('trips', 'NOUN'),\n",
       " ('tweets', 'NOUN'),\n",
       " ('invited', 'VERB'),\n",
       " ('interact', 'NOUN'),\n",
       " ('firehose', 'NOUN'),\n",
       " ('FIFA', 'NOUN'),\n",
       " ('tournament', 'NOUN'),\n",
       " ('NASA', 'NOUN'),\n",
       " ('domineering', 'VERB'),\n",
       " ('Satellite', 'NOUN'),\n",
       " ('personality', 'NOUN'),\n",
       " ('arriving', 'VERB'),\n",
       " ('ICESAT-2', 'NOUN'),\n",
       " ('21st', 'NUM'),\n",
       " ('Android', 'NOUN'),\n",
       " ('OS', 'NOUN'),\n",
       " ('Twitter', 'NOUN'),\n",
       " ('contested', 'VERB'),\n",
       " ('messages', 'NOUN'),\n",
       " ('2011', 'NUM'),\n",
       " ('2015', 'NUM'),\n",
       " ('Google', 'NOUN')]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see the POS tags generated by Viterbi modified with Lexicon & Rules based approach \n",
    "# on Uknown words from test Sentence file (not present in treebank train set) \n",
    "\n",
    "rules_tag_unknown_word = [pair for word in unknown_test_words for pair in test_rules_tagged_seq if word == pair[0]]\n",
    "list(set(rules_tag_unknown_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Google', 'X'),\n",
       " ('worldwide', 'NOUN'),\n",
       " ('Twitter', 'VERB'),\n",
       " ('Cup', 'NOUN'),\n",
       " ('2011', 'DET'),\n",
       " ('online', 'NOUN'),\n",
       " ('2013', 'DET'),\n",
       " ('trips', 'NOUN'),\n",
       " ('ICESAT-2', 'DET'),\n",
       " ('arriving', 'NOUN'),\n",
       " ('interact', 'NOUN'),\n",
       " ('tweets', 'DET'),\n",
       " ('Google', 'DET'),\n",
       " ('FIFA', 'NOUN'),\n",
       " ('tournament', 'NOUN'),\n",
       " ('NASA', 'NOUN'),\n",
       " ('Satellite', 'NOUN'),\n",
       " ('personality', 'NOUN'),\n",
       " ('invited', 'NOUN'),\n",
       " ('2015', 'DET'),\n",
       " ('contested', 'NOUN'),\n",
       " ('firehose', 'VERB'),\n",
       " ('2018', 'NOUN'),\n",
       " ('Android', 'NOUN'),\n",
       " ('OS', 'NOUN'),\n",
       " ('domineering', 'NOUN'),\n",
       " ('Twitter', 'NOUN'),\n",
       " ('21st', 'NOUN'),\n",
       " ('smartphones', 'DET'),\n",
       " ('Google', 'NOUN'),\n",
       " ('messages', 'DET')]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see the POS tags generated by Viterbi modified with Probabilistic approach \n",
    "# on Uknown words from test Sentence file (not present in treebank train set) \n",
    "\n",
    "prob2_tag_unknown_word = [pair for word in unknown_test_words for pair in test_prob2_tagged_seq if word == pair[0]]\n",
    "list(set(prob2_tag_unknown_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "\n",
    "- Clearly unknown words in Train set like 'Twitter', 'Google', 'NASA', 'Android' are all getting wrongly tagged as ADP in Vanilla Viterbi model But same were correctly getting tagged as NOUN by applying modifications\n",
    "\n",
    "- Also Numbers in vanilla viterbi are tagged as ADP but correctly tagged as NUM in Rules based viterbi although they were incorrectly getting tagged as DET in Probabilistic based approach\n",
    "\n",
    "- also rules based modification on vanilla viterbi was able to resolve some words ending with 'ing' as VERB however here is the limitation of rules based approach as it doesn't captures the dependencies of words with other words in sentences hence limited in capturing contecxt hidden in words\n",
    "\n",
    "- Overall Lexicon & Rules based Viterbi model was able to perform best as a POS_Tagger compared to Vanilla Viterbi tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
